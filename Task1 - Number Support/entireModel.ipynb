{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4bdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.12.1-git20200711.33e2d80-dfsg1-0.6 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from math import sqrt, sin, cos\n",
    "from sys import exit\n",
    "import csv\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import pad_sequences \n",
    "\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293d8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-6\n",
    "BATCH_SIZE = 512\n",
    "WEIGHT_DECAY = 1e-6\n",
    "EPOCHS = 1\n",
    "N_JOBS = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i+1] = cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x *= sqrt(self.d_model)\n",
    "        x += self.pe[:,:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbb9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRANSFORMER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 d_model=512,\n",
    "                 nhead=8,\n",
    "                 num_layers=6,\n",
    "                 dropout=0.1,\n",
    "                 use_scheduler=True,\n",
    "                 total_steps=1024,\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, 13)\n",
    "        self.use_scheduler = use_scheduler\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(num_embeddings=input_dim+1, \n",
    "                                          embedding_dim=d_model,\n",
    "                                          padding_idx=0)\n",
    "        \n",
    "        self.dec_embedding = nn.Embedding(num_embeddings=13,  \n",
    "                                          embedding_dim=d_model,\n",
    "                                          padding_idx=0)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoder(d_model=d_model)\n",
    "        \n",
    "        self.transformer_model = nn.Transformer(nhead=nhead, \n",
    "                                                num_encoder_layers=num_layers, \n",
    "                                                num_decoder_layers = num_layers)\n",
    "        \n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "        \n",
    "        ## Hyperparameters ##\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        self.total_steps = total_steps\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        ## steps ##\n",
    "        if self.use_scheduler: \n",
    "            self.total_steps = len(train_dataset) // self.batch_size\n",
    "\n",
    "\n",
    "    # create the dataloaders\n",
    "    # add shuffle only for train_dataloader\n",
    "    # make sure num_workers is set appropriately and drop_last is set to False\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids1, input_ids2):\n",
    "        out1 = self.enc_embedding(input_ids1)\n",
    "        out1 = self.pos_encoder(out1)\n",
    "        \n",
    "        out2 = self.dec_embedding(input_ids2)\n",
    "        out2 = self.pos_encoder(out2)\n",
    "        \n",
    "        tgt_mask = torch.triu(torch.ones(out2.size(0), out2.size(0)), \n",
    "                              diagonal=1).bool().cuda()\n",
    "        \n",
    "        out = self.transformer_model(out1, out2, tgt_mask=tgt_mask)\n",
    "        out = self.fc(out)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids1, ids2 = batch\n",
    "        preds = torch.transpose(self(ids1, ids2), -1, -2)\n",
    "        loss = self.loss_fn(preds, ids2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):           \n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                          lr=self.learning_rate,\n",
    "                          weight_decay=self.weight_decay)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                                        num_warmup_steps=0,\n",
    "                                                        num_training_steps=self.total_steps)\n",
    "            lr_scheduler = {\n",
    "                'scheduler': scheduler, \n",
    "                'interval': 'epoch', \n",
    "                'frequency': 1\n",
    "            }\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946ec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    # open .tsv file\n",
    "    with open(data, 'r', encoding=\"utf-8\") as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for line in tsv_file:\n",
    "            X_train.append(line[0])\n",
    "            y_train.append(line[1])\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e84ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_X(inp):\n",
    "    return [f\"< {re.sub(',', '', re.sub('-', ' ', w))} >\" for w in inp]\n",
    "\n",
    "def preprocess_Y(inp):\n",
    "    return [list(map(int, list(w))) for w in inp]\n",
    "\n",
    "def vocab_creation(inp):\n",
    "    source_vocab = []\n",
    "\n",
    "    #collecting source vocabulary\n",
    "    for num_word in inp:\n",
    "        for word in num_word.split(\" \"):\n",
    "            source_vocab.append(word)\n",
    "\n",
    "    return list(set(source_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c92923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_data(\"./DataGenerationFiles/num_word_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb75b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_X(X_train)\n",
    "y_train = preprocess_Y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb553dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a subset of data to check for the working of model quickly\n",
    "val_X_train = X_train[:1000]\n",
    "val_y_train = y_train[:1000]\n",
    "X_train = X_train[1000:11000]\n",
    "y_train = y_train[1000:11000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2119cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72bfc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['< सत्तर दो सौ पचासी नौ सात एक >', '< नौ छः >', '< शून्य पचासी शून्य उनतीस >', '< एक पाँच पाँच चार छः >', '< पाँच सौ छब्बीस शून्य एक नौ नौ आठ शून्य >']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "269f287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 0, 2, 8, 5, 9, 7, 1], [9, 6], [0, 8, 5, 0, 2, 9], [1, 5, 5, 4, 6], [5, 2, 6, 0, 1, 9, 9, 8, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f931db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = vocab_creation(X_train)\n",
    "source_vocab_dict = dict((v, k) for (k, v) in enumerate(source_vocab, start=1))\n",
    "\n",
    "X_train = [[source_vocab_dict[w] for w in line.split()] for line in X_train]\n",
    "X_train = pad_sequences(X_train, padding='post', value=0)\n",
    "val_X_train = [[source_vocab_dict[w] for w in line.split()] for line in val_X_train]\n",
    "val_X_train = pad_sequences(val_X_train, padding='post', value=0)\n",
    "\n",
    "y_train = [[y+3 for y in w] for w in y_train]\n",
    "y_train = [([1] + w + [2]) for w in y_train]\n",
    "y_train = pad_sequences(y_train, padding='post', value=0)\n",
    "val_y_train = [[y+3 for y in w] for w in val_y_train]\n",
    "val_y_train = [([1] + w + [2]) for w in val_y_train]\n",
    "val_y_train = pad_sequences(val_y_train, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf0b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39  92  58  92  64  58  28  58  26  44   0   0]\n",
      " [ 39  28  92  93  63  26  64  55  92  44   0   0]\n",
      " [ 39  64  62  12  28  58  59   6  64 103  26  44]\n",
      " [ 39  63   6  44   0   0   0   0   0   0   0   0]\n",
      " [ 39  28  93  59  59  58  28  44   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(val_X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6e76418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.LongTensor(X_train), \n",
    "                        torch.LongTensor(y_train))\n",
    "\n",
    "val_dataset = TensorDataset(torch.LongTensor(val_X_train),\n",
    "                           torch.LongTensor(val_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16b18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = TRANSFORMER(input_dim=len(source_vocab_dict),\n",
    "                    train_dataset=dataset,\n",
    "                    val_dataset = val_dataset,\n",
    "                    use_scheduler=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=16,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0729f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/sanjanaa/sanjanaa/jupyter_dir/Task1 - Number Support/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0 | fc                | Linear            | 6.7 K \n",
      "1 | enc_embedding     | Embedding         | 54.3 K\n",
      "2 | dec_embedding     | Embedding         | 6.7 K \n",
      "3 | pos_encoder       | PositionalEncoder | 0     \n",
      "4 | transformer_model | Transformer       | 44.1 M\n",
      "5 | loss_fn           | NLLLoss           | 0     \n",
      "--------------------------------------------------------\n",
      "44.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.2 M    Total params\n",
      "88.416    Total estimated model params size (MB)\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0031242bc84e309a7753ff2bc52103"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9335fbf8841489796ad982f39c7d31a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
