{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d4bdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from math import sqrt, sin, cos\n",
    "from sys import exit\n",
    "import csv\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import pad_sequences \n",
    "\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6977fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 7.5e-4\n",
    "BATCH_SIZE = 1024\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EPOCHS = 100\n",
    "N_JOBS = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f7a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=512, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i+1] = cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x *= sqrt(self.d_model)\n",
    "        x += self.pe[:,:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0fbb9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRANSFORMER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 d_model=512,\n",
    "                 nhead=8,\n",
    "                 num_layers=6,\n",
    "                 dropout=0.1,\n",
    "                 use_scheduler=True,\n",
    "                 total_steps=1024,\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None,\n",
    "                 activation='gelu',\n",
    "                 batch_first=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.count_acc = 0\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, 13)\n",
    "        self.use_scheduler = use_scheduler\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(num_embeddings=input_dim+1, \n",
    "                                          embedding_dim=d_model,\n",
    "                                          padding_idx=0)\n",
    "        \n",
    "        self.dec_embedding = nn.Embedding(num_embeddings=13,  \n",
    "                                          embedding_dim=d_model,\n",
    "                                          padding_idx=0)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoder(d_model=d_model)\n",
    "        \n",
    "        self.transformer_model = nn.Transformer(nhead=nhead, \n",
    "                                                num_encoder_layers=num_layers, \n",
    "                                                num_decoder_layers = num_layers)\n",
    "        \n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "        \n",
    "        self.best_val_acc = 0\n",
    "        \n",
    "        self.train_loss = 0\n",
    "        self.train_steps = 0\n",
    "        \n",
    "        self.val_loss = 0\n",
    "        self.val_steps = 0\n",
    "        ## Hyperparameters ##\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        self.total_steps = total_steps\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        ## steps ##\n",
    "        if self.use_scheduler: \n",
    "            self.total_steps = len(train_dataset) // self.batch_size\n",
    "\n",
    "\n",
    "    # create the dataloaders\n",
    "    # add shuffle only for train_dataloader\n",
    "    # make sure num_workers is set appropriately and drop_last is set to False\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids1, input_ids2):\n",
    "        out1 = self.enc_embedding(input_ids1)\n",
    "        out1 = self.pos_encoder(out1)\n",
    "        #print(out1.shape)\n",
    "        out1 = torch.permute(out1, (1,0,2))\n",
    "        #print(out1.shape)\n",
    "        \n",
    "        out2 = self.dec_embedding(input_ids2)\n",
    "        out2 = self.pos_encoder(out2)\n",
    "        #print(out2.shape)\n",
    "        out2 = torch.permute(out2, (1,0,2))\n",
    "        #print(out2.shape)\n",
    "        \n",
    "        tgt_mask = torch.triu(torch.ones(out2.size(0), out2.size(0)), \n",
    "                              diagonal=1).bool().cuda()\n",
    "        \n",
    "        out = self.transformer_model(out1, out2, tgt_mask=tgt_mask)\n",
    "        out = self.fc(out)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "    def count_correct(self, pred, true):\n",
    "    \n",
    "        pred_nums = torch.argmax(pred, dim = 1)\n",
    "        for i in range(pred_nums.shape[0]):\n",
    "            if torch.eq(pred_nums[i], true[i]).all():\n",
    "                self.count_acc += 1\n",
    "            \n",
    "\n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids1, ids2 = batch\n",
    "        preds = self(ids1,ids2)\n",
    "        \n",
    "        preds = torch.permute(preds, (1,2,0))\n",
    "        \n",
    "        self.count_correct(preds, ids2)\n",
    "        \n",
    "        loss = self.loss_fn(preds, ids2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.train_loss += loss\n",
    "        self.train_steps += 1\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, data_len):\n",
    "        return self.count_acc/data_len\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        acc = self.accuracy(len(self.train_dataset))\n",
    "        #loss = sum(output['loss'] for output in outputs) / len(outputs)\n",
    "        print(\"Training loss: \", self.train_loss/self.train_steps)\n",
    "        print(\"Training accuracy: \", acc)\n",
    "        self.count_acc = 0\n",
    "        self.train_loss = 0\n",
    "        self.train_steps = 0\n",
    "        \n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        \n",
    "        acc = self.accuracy(len(self.val_dataset))\n",
    "        print(\"EPOCH\")\n",
    "        #loss = sum(output['loss'] for output in outputs) / len(outputs)\n",
    "        print(\"Validation loss: \", self.val_loss/self.val_steps)\n",
    "        if acc > self.best_val_acc:\n",
    "            print(\"Current best model. Saving at epoch number: \", self.current_epoch)\n",
    "            PATH = \"model_\"+str(self.current_epoch)+\".pt\"\n",
    "            torch.save({\n",
    "                'epoch': self.current_epoch,\n",
    "                'model_state_dict': self.state_dict(),\n",
    "                'optimizer_state_dict': self.opt.state_dict(),\n",
    "                'loss': loss,\n",
    "                'val_accuracy': acc\n",
    "            }, PATH)\n",
    "        print(\"Validation accuracy: \", acc)\n",
    "        self.count_acc = 0\n",
    "        self.val_loss = 0\n",
    "        self.val_steps = 0\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.val_loss += loss\n",
    "        self.val_steps += 1\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):           \n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                          lr=self.learning_rate,\n",
    "                          weight_decay=self.weight_decay)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                                        num_warmup_steps=1,\n",
    "                                                        num_training_steps=self.total_steps)\n",
    "            lr_scheduler = {\n",
    "                'scheduler': scheduler, \n",
    "                'interval': 'epoch', \n",
    "                'frequency': 1\n",
    "            }\n",
    "            self.opt_state_dict = optimizer.state_dict()\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            self.opt_state_dict = optimizer.state_dict()\n",
    "            return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b533457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ab7313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis = [[[3,2,1,2,13],\n",
    "#         [3,6,8,1,5],\n",
    "#         [4,23,6,4,7],\n",
    "#         [7,6,3,0,5]],\n",
    "#        [[3,2,1,2,13],\n",
    "#         [3,6,8,1,5],\n",
    "#         [4,23,6,4,7],\n",
    "#         [7,6,3,0,5]],\n",
    "#       [[3,2,1,2,13],\n",
    "#         [3,6,8,1,5],\n",
    "#         [4,23,6,4,7],\n",
    "#         [7,6,3,0,5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dced4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lis2 = [[3, 1, 1, 0, 0],\n",
    "#         [3, 2, 0, 2, 0],\n",
    "#         [3, 2, 0, 2, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bc79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy(torch.tensor(lis), torch.tensor(lis2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b45b4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.shape(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dadfe7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = torch.argmax(torch.LongTensor(lis), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb888d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64d52223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "946ec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    # open .tsv file\n",
    "    with open(data, 'r', encoding=\"utf-8\") as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for line in tsv_file:\n",
    "            X_train.append(line[0])\n",
    "            y_train.append(line[1])\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e84ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_X(inp):\n",
    "    return [f\"< {re.sub(',', '', re.sub('-', ' ', w))} >\" for w in inp]\n",
    "\n",
    "def preprocess_Y(inp):\n",
    "    return [list(map(int, list(w))) for w in inp]\n",
    "\n",
    "def vocab_creation(inp):\n",
    "    source_vocab = []\n",
    "\n",
    "    #collecting source vocabulary\n",
    "    for num_word in inp:\n",
    "        for word in num_word.split(\" \"):\n",
    "            source_vocab.append(word)\n",
    "\n",
    "    return list(set(source_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c92923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = read_data(\"./DataGenerationFiles/num_word_data.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cdb75b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_X(X_train)\n",
    "y_train = preprocess_Y(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dfb8bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a subset of data to check for the working of model quickly\n",
    "val_X_train = X_train[:500]\n",
    "val_y_train = y_train[:500]\n",
    "X_train = X_train[500:]\n",
    "y_train = y_train[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec29233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c720b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['< आठ सौ अट्ठाईस दो >', '< पाँच तीन शून्य तीन >', '< एक पाँच पाँच नौ नौ पाँच चौरानवे >', '< चार शून्य छः सात आठ दो दो पाँच नौ >', '< नौ चार छः तीन आठ एक आठ >']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffb077c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 2, 8, 2], [5, 3, 0, 3], [1, 5, 5, 9, 9, 5, 9, 4], [4, 0, 6, 7, 8, 2, 2, 5, 9], [9, 4, 6, 3, 8, 1, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f931db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = vocab_creation(X_train)\n",
    "source_vocab_dict = dict((v, k) for (k, v) in enumerate(source_vocab, start=1))\n",
    "\n",
    "X_train = [[source_vocab_dict[w] for w in line.split()] for line in X_train]\n",
    "X_train = pad_sequences(X_train, padding='post', value=0)\n",
    "val_X_train = [[source_vocab_dict[w] for w in line.split()] for line in val_X_train]\n",
    "val_X_train = pad_sequences(val_X_train, padding='post', value=0)\n",
    "\n",
    "y_train = [[y+3 for y in w] for w in y_train]\n",
    "y_train = [([1] + w + [2]) for w in y_train]\n",
    "y_train = pad_sequences(y_train, padding='post', value=0)\n",
    "val_y_train = [[y+3 for y in w] for w in val_y_train]\n",
    "val_y_train = [([1] + w + [2]) for w in val_y_train]\n",
    "val_y_train = pad_sequences(val_y_train, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b92e6031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'चालीस': 1, 'बयालीस': 2, 'सोलह': 3, 'तीन': 4, 'निन्यानवे': 5, 'पैंतालीस': 6, 'चौंतालीस': 7, 'उन्नीस': 8, 'सत्तावन': 9, 'सैंतीस': 10, 'पचासी': 11, 'तिरेपन': 12, 'इकसठ': 13, 'अस्सी': 14, 'तिरेसठ': 15, 'अड़तालीस': 16, 'बाईस': 17, 'बानवे': 18, 'पंद्रह': 19, 'चौहत्तर': 20, 'उनहत्तर': 21, 'सत्ताईस': 22, 'पचहत्तर': 23, '>': 24, 'पच्चीस': 25, 'सतहत्तर': 26, 'सात': 27, 'बयासी': 28, 'छियालीस': 29, 'इकतालीस': 30, 'चौबिस': 31, 'सत्तानवे': 32, 'सैंतालीस': 33, 'नवासी': 34, 'सत्तर': 35, 'तैंतीस': 36, 'चौवन': 37, 'बासठ': 38, 'बारह': 39, 'ग्यारह': 40, 'अड़सठ': 41, 'छब्बीस': 42, 'पैंसठ': 43, 'चार': 44, 'चौंतीस': 45, 'डबल': 46, 'सरसठ\\u200b': 47, 'बत्तीस': 48, 'उनतीस': 49, 'छियानवे': 50, 'आठ': 51, 'साठ': 52, 'सत्रह': 53, 'चौदह': 54, 'उनसठ': 55, 'सत्तासी': 56, 'अट्ठानवे': 57, 'चौंसठ': 58, 'छिहत्तर': 59, 'छत्तीस': 60, 'ट्रिपल': 61, 'छः': 62, 'सौ': 63, 'तेईस': 64, 'छप्पन': 65, 'इक्यावन\\u200b': 66, 'अड़तीस': 67, 'चौरासी': 68, 'पैंतीस': 69, 'नौ': 70, 'दो': 71, 'अट्ठावन': 72, 'नब्बे': 73, 'चौरानवे': 74, 'तिरासी': 75, 'इकहत्तर': 76, 'पचानवे': 77, 'बीस': 78, '<': 79, 'बावन': 80, 'शून्य': 81, 'तिहत्तर': 82, 'दस': 83, 'पाँच': 84, 'इकतीस': 85, 'तिरानवे': 86, 'अठासी': 87, 'तैंतालीस': 88, 'इक्यासी': 89, 'उन्यासी': 90, 'एक': 91, 'छयासठ': 92, 'इक्कीस': 93, 'पचास': 94, 'बहत्तर': 95, 'पचपन': 96, 'उनचास': 97, 'अठहत्तर': 98, 'छियासी': 99, 'अट्ठारह': 100, 'उनतालीस': 101, 'इक्यानवे': 102, 'तेरह': 103, 'तीस': 104, 'अट्ठाईस': 105}\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "672d972c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 6, 3, 6, 2, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c7f9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79 51 70 51 44 70 84 70 62 24  0  0]\n",
      " [79 84 51 46 71 62 44 27 51 24  0  0]\n",
      " [79 44 63 16 84 70 81  4 44 91 62 24]\n",
      " [79 71  4 24  0  0  0  0  0  0  0  0]\n",
      " [79 84 46 81 81 70 84 24  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(val_X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c6e76418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.LongTensor(X_train), \n",
    "                        torch.LongTensor(y_train))\n",
    "\n",
    "val_dataset = TensorDataset(torch.LongTensor(val_X_train),\n",
    "                           torch.LongTensor(val_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "728c2126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999500"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e16b18d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = TRANSFORMER(input_dim=len(source_vocab_dict),\n",
    "                    train_dataset=dataset,\n",
    "                    val_dataset = val_dataset,\n",
    "                    use_scheduler=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=32,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a899dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type              | Params\n",
      "--------------------------------------------------------\n",
      "0 | fc                | Linear            | 6.7 K \n",
      "1 | enc_embedding     | Embedding         | 54.3 K\n",
      "2 | dec_embedding     | Embedding         | 6.7 K \n",
      "3 | pos_encoder       | PositionalEncoder | 0     \n",
      "4 | transformer_model | Transformer       | 44.1 M\n",
      "5 | loss_fn           | NLLLoss           | 0     \n",
      "--------------------------------------------------------\n",
      "44.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.2 M    Total params\n",
      "176.833   Total estimated model params size (MB)\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751cc6df46454ae4bad83b524482fff4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0db43329",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = TRANSFORMER(input_dim=len(source_vocab_dict),\n",
    "                    train_dataset=dataset,\n",
    "                    val_dataset = val_dataset,\n",
    "                    use_scheduler=True)\n",
    "saved_model.load_state_dict(torch.load('model1.pth'))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cca3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ca9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(saved_model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, input_sequence, max_length=12, SOS_token=1, EOS_token=2):\n",
    "    \n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=\"cuda\")\n",
    "    #print(y_input.shape)\n",
    "    #y_input = torch.permute(y_input, (1,0))\n",
    "    #print(input_sequence)\n",
    "    #input_sequence = torch.permute(input_sequence, (1,0))\n",
    "    num_tokens = len(input_sequence)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        #tgt_mask = get_tgt_mask(y_input.size(1)).to(\"cuda\")\n",
    "        \n",
    "        pred = model(input_sequence, y_input)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=\"cuda\")\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "bism = inference(saved_model, torch.tensor([val_X_train[1]], dtype=torch.long, device=\"cuda\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4d212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
